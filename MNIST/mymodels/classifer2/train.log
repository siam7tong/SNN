2017-12-20 09:55:15.533693: softmax classifer
2017-12-20 09:55:15.533780: Learning rates LR: 1.000000 
2017-12-20 09:56:38.675910: Iter: 2000 [0], loss: 307198.125000, acc: 0.00%, avg_loss: 120826.412255, avg_acc: 29.60%
2017-12-20 09:58:05.280261: Iter: 4000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 113516.877322, avg_acc: 33.67%
2017-12-20 09:59:25.988740: Iter: 6000 [0], loss: 77312.835938, acc: 0.00%, avg_loss: 110686.839760, avg_acc: 36.30%
2017-12-20 10:00:48.261123: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 106252.944307, avg_acc: 38.49%
2017-12-20 10:02:09.958832: Iter: 10000 [0], loss: 521231.718750, acc: 0.00%, avg_loss: 103992.826080, avg_acc: 40.53%
2017-12-20 10:03:34.763215: Iter: 12000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 102151.218568, avg_acc: 42.19%
2017-12-20 10:03:34.763597: 
Epoch 0: avg_Loss: 102159.731878616993, avg_Acc: 42.195182931911
2017-12-20 10:03:34.763639: Learning rates changed LR: 1.000000 
2017-12-20 10:05:00.758816: Iter: 14000 [1], loss: 116808.460938, acc: 0.00%, avg_loss: 93607.715145, avg_acc: 49.65%
2017-12-20 10:06:28.399261: Iter: 16000 [1], loss: 649.187500, acc: 0.00%, avg_loss: 83329.184167, avg_acc: 52.52%
2017-12-20 10:07:51.620060: Iter: 18000 [1], loss: 248145.484375, acc: 0.00%, avg_loss: 81818.516826, avg_acc: 53.37%
2017-12-20 10:09:12.969530: Iter: 20000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 80865.678697, avg_acc: 53.74%
2017-12-20 10:10:34.905130: Iter: 22000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 82071.347807, avg_acc: 53.78%
2017-12-20 10:11:57.016360: Iter: 24000 [1], loss: 237316.515625, acc: 0.00%, avg_loss: 82447.368158, avg_acc: 54.22%
2017-12-20 10:11:57.016657: 
Epoch 1: avg_Loss: 82454.239344571135, avg_Acc: 54.221185098758
2017-12-20 10:11:57.016693: Learning rates changed LR: 0.800000 
2017-12-20 10:13:20.043876: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 61478.689086, avg_acc: 62.10%
2017-12-20 10:14:40.892955: Iter: 28000 [2], loss: 393848.312500, acc: 0.00%, avg_loss: 62593.164181, avg_acc: 61.60%
2017-12-20 10:16:04.227344: Iter: 30000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 63020.929863, avg_acc: 61.20%
2017-12-20 10:17:27.595766: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 63077.164503, avg_acc: 60.95%
2017-12-20 10:18:54.285197: Iter: 34000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 62596.411309, avg_acc: 61.02%
2017-12-20 10:20:17.318735: Iter: 36000 [2], loss: 714.390625, acc: 0.00%, avg_loss: 62261.344854, avg_acc: 61.09%
2017-12-20 10:20:17.319035: 
Epoch 2: avg_Loss: 62266.533731337891, avg_Acc: 61.096758063172
2017-12-20 10:20:17.319070: Learning rates changed LR: 0.100000 
2017-12-20 10:21:39.311341: Iter: 38000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 23005.201924, avg_acc: 76.15%
2017-12-20 10:23:02.739488: Iter: 40000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 22736.368179, avg_acc: 76.50%
2017-12-20 10:24:30.165791: Iter: 42000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 24127.994069, avg_acc: 76.07%
2017-12-20 10:25:58.086423: Iter: 44000 [3], loss: 19178.625000, acc: 0.00%, avg_loss: 24165.943782, avg_acc: 76.02%
2017-12-20 10:27:22.054249: Iter: 46000 [3], loss: 177058.656250, acc: 0.00%, avg_loss: 24112.816481, avg_acc: 76.04%
2017-12-20 10:28:46.083319: Iter: 48000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 23484.547245, avg_acc: 76.24%
2017-12-20 10:28:46.083652: 
Epoch 3: avg_Loss: 23486.504453268444, avg_Acc: 76.248020668389
2017-12-20 10:28:46.083753: Learning rates changed LR: 0.010000 
2017-12-20 10:30:09.892270: Iter: 50000 [4], loss: 9366.875000, acc: 0.00%, avg_loss: 21787.961285, avg_acc: 77.15%
2017-12-20 10:31:36.896022: Iter: 52000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 20857.274032, avg_acc: 77.92%
2017-12-20 10:33:02.341725: Iter: 54000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 20383.864511, avg_acc: 78.10%
2017-12-20 10:34:24.644526: Iter: 56000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 20073.514698, avg_acc: 78.01%
2017-12-20 10:35:47.140299: Iter: 58000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 20240.681764, avg_acc: 78.06%
2017-12-20 10:37:11.733437: Iter: 60000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 19883.241314, avg_acc: 78.33%
2017-12-20 10:37:11.733672: 
Epoch 4: avg_Loss: 19884.898389319776, avg_Acc: 78.331527627302
2017-12-20 10:37:11.733709: Learning rates changed LR: 0.001000 
2017-12-20 10:38:41.183413: Iter: 62000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 18197.470076, avg_acc: 79.40%
2017-12-20 10:40:07.714596: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 18597.421187, avg_acc: 78.80%
2017-12-20 10:41:39.236288: Iter: 66000 [5], loss: 87509.156250, acc: 0.00%, avg_loss: 19343.797669, avg_acc: 78.13%
2017-12-20 10:43:02.023565: Iter: 68000 [5], loss: 2268.468750, acc: 0.00%, avg_loss: 19525.549633, avg_acc: 78.16%
2017-12-20 10:44:27.446234: Iter: 70000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 19540.495293, avg_acc: 78.36%
2017-12-20 10:45:55.687838: Iter: 72000 [5], loss: 7991.203125, acc: 0.00%, avg_loss: 19458.476607, avg_acc: 78.35%
2017-12-20 10:45:55.688114: 
Epoch 5: avg_Loss: 19460.098281627750, avg_Acc: 78.356529710809
2017-12-20 10:45:55.688150: Learning rates changed LR: 0.000000 
2017-12-20 10:45:56.448169: 
Testing at last epoch...
2017-12-20 10:52:56.207379: epoch: 6 Accuracy: 78.7800%, loss: 18764.547682812499, i: 10000
2017-12-20 10:52:56.207457: Exiting train...
