2017-12-28 11:34:43.571242: softmax classifer
2017-12-28 11:34:43.571312: Learning rates LR: 1.000000 
2017-12-28 11:36:02.141952: Iter: 2000 [0], loss: 10.084433, acc: 0.00%, avg_loss: 2.202075, avg_acc: 67.15%
2017-12-28 11:37:23.533590: Iter: 4000 [0], loss: 0.001669, acc: 1.00%, avg_loss: 2.188330, avg_acc: 66.55%
2017-12-28 11:38:43.585728: Iter: 6000 [0], loss: 1.646267, acc: 0.00%, avg_loss: 2.210109, avg_acc: 66.43%
2017-12-28 11:40:02.531239: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 2.201851, avg_acc: 66.72%
2017-12-28 11:41:21.426056: Iter: 10000 [0], loss: 3.055114, acc: 0.00%, avg_loss: 2.211601, avg_acc: 66.53%
2017-12-28 11:42:39.957505: Iter: 12000 [0], loss: 1.440415, acc: 0.00%, avg_loss: 2.188597, avg_acc: 66.67%
2017-12-28 11:42:39.957736: 
Epoch 0: avg_Loss: 2.188779129776, avg_Acc: 66.672222685224
2017-12-28 11:42:39.957764: Learning rates changed LR: 0.100000 
2017-12-28 11:43:59.909390: Iter: 14000 [1], loss: 1.829277, acc: 0.00%, avg_loss: 1.074371, avg_acc: 78.20%
2017-12-28 11:45:18.360832: Iter: 16000 [1], loss: 2.754765, acc: 0.00%, avg_loss: 1.095526, avg_acc: 78.20%
2017-12-28 11:46:36.972216: Iter: 18000 [1], loss: 0.190372, acc: 1.00%, avg_loss: 1.074067, avg_acc: 78.47%
2017-12-28 11:47:56.393573: Iter: 20000 [1], loss: 14.174088, acc: 0.00%, avg_loss: 1.099685, avg_acc: 78.29%
2017-12-28 11:49:15.820641: Iter: 22000 [1], loss: 0.000007, acc: 1.00%, avg_loss: 1.083802, avg_acc: 78.59%
2017-12-28 11:50:35.706850: Iter: 24000 [1], loss: 0.000675, acc: 1.00%, avg_loss: 1.114391, avg_acc: 78.09%
2017-12-28 11:50:35.707126: 
Epoch 1: avg_Loss: 1.114483590127, avg_Acc: 78.098174847904
2017-12-28 11:50:35.707157: Learning rates changed LR: 0.010000 
2017-12-28 11:51:55.769400: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1.063380, avg_acc: 79.75%
2017-12-28 11:53:15.851140: Iter: 28000 [2], loss: 0.000038, acc: 1.00%, avg_loss: 1.046871, avg_acc: 79.75%
2017-12-28 11:54:34.600630: Iter: 30000 [2], loss: 1.825353, acc: 0.00%, avg_loss: 1.032013, avg_acc: 79.65%
2017-12-28 11:55:53.737807: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1.032538, avg_acc: 79.62%
2017-12-28 11:57:12.536959: Iter: 34000 [2], loss: 0.000270, acc: 1.00%, avg_loss: 1.035948, avg_acc: 79.42%
2017-12-28 11:58:31.994832: Iter: 36000 [2], loss: 0.000030, acc: 1.00%, avg_loss: 1.038405, avg_acc: 79.19%
2017-12-28 11:58:31.995120: 
Epoch 2: avg_Loss: 1.038491804120, avg_Acc: 79.198266522210
2017-12-28 11:58:31.995150: Learning rates changed LR: 0.010000 
2017-12-28 11:59:51.951411: Iter: 38000 [3], loss: 0.000064, acc: 1.00%, avg_loss: 0.971592, avg_acc: 79.60%
2017-12-28 12:01:11.785970: Iter: 40000 [3], loss: 0.000226, acc: 1.00%, avg_loss: 1.026451, avg_acc: 79.55%
2017-12-28 12:02:33.512686: Iter: 42000 [3], loss: 0.000484, acc: 1.00%, avg_loss: 1.020631, avg_acc: 79.30%
2017-12-28 12:03:54.187083: Iter: 44000 [3], loss: 0.000002, acc: 1.00%, avg_loss: 1.038592, avg_acc: 78.71%
2017-12-28 12:05:15.330319: Iter: 46000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1.023922, avg_acc: 79.06%
2017-12-28 12:06:35.553989: Iter: 48000 [3], loss: 0.000322, acc: 1.00%, avg_loss: 1.032600, avg_acc: 79.05%
2017-12-28 12:06:35.554186: 
Epoch 3: avg_Loss: 1.032685672619, avg_Acc: 79.056588049004
2017-12-28 12:06:35.554215: Learning rates changed LR: 0.001000 
2017-12-28 12:07:55.318673: Iter: 50000 [4], loss: 0.001379, acc: 1.00%, avg_loss: 0.997421, avg_acc: 79.85%
2017-12-28 12:09:14.471231: Iter: 52000 [4], loss: 2.689481, acc: 0.00%, avg_loss: 1.039021, avg_acc: 79.03%
2017-12-28 12:10:33.232356: Iter: 54000 [4], loss: 0.006591, acc: 1.00%, avg_loss: 1.004463, avg_acc: 79.15%
2017-12-28 12:11:53.335829: Iter: 56000 [4], loss: 0.000006, acc: 1.00%, avg_loss: 1.019347, avg_acc: 78.90%
2017-12-28 12:13:12.534377: Iter: 58000 [4], loss: 4.122496, acc: 0.00%, avg_loss: 0.996792, avg_acc: 79.33%
2017-12-28 12:14:34.329764: Iter: 60000 [4], loss: 0.670900, acc: 1.00%, avg_loss: 1.023868, avg_acc: 79.14%
2017-12-28 12:14:34.329961: 
Epoch 4: avg_Loss: 1.023953548500, avg_Acc: 79.148262355196
2017-12-28 12:14:34.329986: Learning rates changed LR: 0.001000 
2017-12-28 12:15:54.092383: Iter: 62000 [5], loss: 3.433959, acc: 0.00%, avg_loss: 1.042538, avg_acc: 79.15%
2017-12-28 12:17:20.582119: Iter: 64000 [5], loss: 0.000380, acc: 1.00%, avg_loss: 1.045211, avg_acc: 79.35%
2017-12-28 12:18:40.278246: Iter: 66000 [5], loss: 0.000036, acc: 1.00%, avg_loss: 1.031287, avg_acc: 79.62%
2017-12-28 12:20:01.919174: Iter: 68000 [5], loss: 3.376063, acc: 0.00%, avg_loss: 1.047570, avg_acc: 79.10%
2017-12-28 12:21:20.326247: Iter: 70000 [5], loss: 0.000003, acc: 1.00%, avg_loss: 1.026568, avg_acc: 79.26%
2017-12-28 12:22:39.003836: Iter: 72000 [5], loss: 0.000030, acc: 1.00%, avg_loss: 1.023327, avg_acc: 79.16%
2017-12-28 12:22:39.004080: 
Epoch 5: avg_Loss: 1.023412555534, avg_Acc: 79.164930410868
2017-12-28 12:22:39.004123: Learning rates changed LR: 0.000100 
2017-12-28 12:23:58.454516: Iter: 74000 [6], loss: 0.000550, acc: 1.00%, avg_loss: 1.020757, avg_acc: 79.85%
2017-12-28 12:25:18.141149: Iter: 76000 [6], loss: 0.000003, acc: 1.00%, avg_loss: 1.025211, avg_acc: 78.97%
2017-12-28 12:26:37.187902: Iter: 78000 [6], loss: 0.001430, acc: 1.00%, avg_loss: 1.039254, avg_acc: 78.85%
2017-12-28 12:28:01.278184: Iter: 80000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1.041783, avg_acc: 78.80%
2017-12-28 12:29:20.250042: Iter: 82000 [6], loss: 0.000016, acc: 1.00%, avg_loss: 1.025716, avg_acc: 79.18%
2017-12-28 12:30:42.289181: Iter: 84000 [6], loss: 0.000398, acc: 1.00%, avg_loss: 1.021974, avg_acc: 79.11%
2017-12-28 12:30:42.289449: 
Epoch 6: avg_Loss: 1.022059419525, avg_Acc: 79.114926243854
2017-12-28 12:30:42.289491: Learning rates changed LR: 0.000100 
2017-12-28 12:32:02.157263: Iter: 86000 [7], loss: 0.000015, acc: 1.00%, avg_loss: 1.049046, avg_acc: 79.05%
2017-12-28 12:33:22.820676: Iter: 88000 [7], loss: 0.000017, acc: 1.00%, avg_loss: 1.042130, avg_acc: 79.17%
2017-12-28 12:34:42.826235: Iter: 90000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1.029651, avg_acc: 79.08%
2017-12-28 12:36:04.692405: Iter: 92000 [7], loss: 0.007761, acc: 1.00%, avg_loss: 1.037834, avg_acc: 78.88%
2017-12-28 12:37:24.688372: Iter: 94000 [7], loss: 0.000015, acc: 1.00%, avg_loss: 1.017406, avg_acc: 79.16%
2017-12-28 12:38:44.623927: Iter: 96000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1.021937, avg_acc: 79.10%
2017-12-28 12:38:44.624221: 
Epoch 7: avg_Loss: 1.022022506879, avg_Acc: 79.106592216018
2017-12-28 12:38:45.464293: 
Testing at last epoch...
2017-12-28 12:45:17.069488: epoch: 8 Accuracy: 79.5600%, loss: 0.989787454376, i: 10000
2017-12-28 12:45:17.069555: Exiting train...
