2017-12-28 10:01:09.862227: softmax classifer
2017-12-28 10:01:09.862317: Learning rates LR: 1.000000 
2017-12-28 10:02:58.694147: Iter: 2000 [0], loss: 0.005839, acc: 1.00%, avg_loss: 5.228569, avg_acc: 21.85%
2017-12-28 10:04:16.470527: Iter: 4000 [0], loss: 0.113882, acc: 1.00%, avg_loss: 4.655042, avg_acc: 27.88%
2017-12-28 10:05:34.549420: Iter: 6000 [0], loss: 1.669306, acc: 0.00%, avg_loss: 4.372506, avg_acc: 31.43%
2017-12-28 10:06:52.717447: Iter: 8000 [0], loss: 0.156105, acc: 1.00%, avg_loss: 4.089163, avg_acc: 34.90%
2017-12-28 10:08:10.500531: Iter: 10000 [0], loss: 0.886908, acc: 0.00%, avg_loss: 3.932258, avg_acc: 37.14%
2017-12-28 10:09:29.095970: Iter: 12000 [0], loss: 0.000122, acc: 1.00%, avg_loss: 3.791382, avg_acc: 39.13%
2017-12-28 10:09:29.096215: 
Epoch 0: avg_Loss: 3.791698317956, avg_Acc: 39.136594716226
2017-12-28 10:10:49.094004: Iter: 14000 [1], loss: 0.455976, acc: 1.00%, avg_loss: 3.192408, avg_acc: 48.85%
2017-12-28 10:12:07.266056: Iter: 16000 [1], loss: 4.023160, acc: 0.00%, avg_loss: 2.967468, avg_acc: 51.52%
2017-12-28 10:13:24.587837: Iter: 18000 [1], loss: 4.771729, acc: 0.00%, avg_loss: 2.928981, avg_acc: 51.85%
2017-12-28 10:14:42.647295: Iter: 20000 [1], loss: 0.013344, acc: 1.00%, avg_loss: 2.878484, avg_acc: 52.42%
2017-12-28 10:16:00.657418: Iter: 22000 [1], loss: 1.623718, acc: 0.00%, avg_loss: 2.864619, avg_acc: 52.99%
2017-12-28 10:17:18.731408: Iter: 24000 [1], loss: 11.114759, acc: 0.00%, avg_loss: 2.840377, avg_acc: 53.41%
2017-12-28 10:17:18.731781: 
Epoch 1: avg_Loss: 2.840613445248, avg_Acc: 53.412784398700
2017-12-28 10:18:38.414810: Iter: 26000 [2], loss: 0.000001, acc: 1.00%, avg_loss: 2.588874, avg_acc: 57.75%
2017-12-28 10:19:56.325281: Iter: 28000 [2], loss: 5.709124, acc: 0.00%, avg_loss: 2.590049, avg_acc: 57.77%
2017-12-28 10:21:14.024486: Iter: 30000 [2], loss: 3.255736, acc: 0.00%, avg_loss: 2.552418, avg_acc: 58.13%
2017-12-28 10:22:32.372132: Iter: 32000 [2], loss: 0.009565, acc: 1.00%, avg_loss: 2.541137, avg_acc: 58.34%
2017-12-28 10:23:50.671062: Iter: 34000 [2], loss: 1.133178, acc: 0.00%, avg_loss: 2.576498, avg_acc: 57.98%
2017-12-28 10:25:08.713143: Iter: 36000 [2], loss: 5.142056, acc: 0.00%, avg_loss: 2.569613, avg_acc: 58.32%
2017-12-28 10:25:08.713552: 
Epoch 2: avg_Loss: 2.569827307693, avg_Acc: 58.321526793899
2017-12-28 10:26:27.930937: Iter: 38000 [3], loss: 0.061390, acc: 1.00%, avg_loss: 2.482594, avg_acc: 58.70%
2017-12-28 10:27:46.358449: Iter: 40000 [3], loss: 0.002829, acc: 1.00%, avg_loss: 2.449171, avg_acc: 59.42%
2017-12-28 10:29:06.285745: Iter: 42000 [3], loss: 0.012752, acc: 1.00%, avg_loss: 2.517918, avg_acc: 59.70%
2017-12-28 10:30:52.060637: Iter: 44000 [3], loss: 0.403955, acc: 1.00%, avg_loss: 2.492488, avg_acc: 60.19%
2017-12-28 10:32:49.827488: Iter: 46000 [3], loss: 4.493853, acc: 0.00%, avg_loss: 2.494662, avg_acc: 60.46%
2017-12-28 10:34:43.776323: Iter: 48000 [3], loss: 9.298954, acc: 0.00%, avg_loss: 2.456433, avg_acc: 60.91%
2017-12-28 10:34:43.776866: 
Epoch 3: avg_Loss: 2.456637820458, avg_Acc: 60.913409450788
2017-12-28 10:36:40.491943: Iter: 50000 [4], loss: 6.788910, acc: 0.00%, avg_loss: 2.467675, avg_acc: 61.35%
2017-12-28 10:38:38.057587: Iter: 52000 [4], loss: 0.019225, acc: 1.00%, avg_loss: 2.426773, avg_acc: 61.48%
2017-12-28 10:40:31.939514: Iter: 54000 [4], loss: 0.000001, acc: 1.00%, avg_loss: 2.391669, avg_acc: 62.12%
2017-12-28 10:42:27.164958: Iter: 56000 [4], loss: 0.058309, acc: 1.00%, avg_loss: 2.370160, avg_acc: 62.35%
2017-12-28 10:44:20.757288: Iter: 58000 [4], loss: 2.336723, acc: 0.00%, avg_loss: 2.384188, avg_acc: 62.34%
2017-12-28 10:46:15.516531: Iter: 60000 [4], loss: 0.687899, acc: 1.00%, avg_loss: 2.354470, avg_acc: 62.58%
2017-12-28 10:46:15.516929: 
Epoch 4: avg_Loss: 2.354666323935, avg_Acc: 62.588549045754
2017-12-28 10:48:06.702708: Iter: 62000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 2.193641, avg_acc: 65.20%
2017-12-28 10:50:04.938758: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 2.227044, avg_acc: 64.50%
2017-12-28 10:52:00.099749: Iter: 66000 [5], loss: 7.782629, acc: 0.00%, avg_loss: 2.289186, avg_acc: 64.25%
2017-12-28 10:53:58.408298: Iter: 68000 [5], loss: 8.452050, acc: 0.00%, avg_loss: 2.306914, avg_acc: 63.96%
2017-12-28 10:55:42.394103: Iter: 70000 [5], loss: 0.703643, acc: 1.00%, avg_loss: 2.298959, avg_acc: 64.25%
2017-12-28 10:57:04.946998: Iter: 72000 [5], loss: 7.143926, acc: 0.00%, avg_loss: 2.289977, avg_acc: 64.38%
2017-12-28 10:57:04.947199: 
Epoch 5: avg_Loss: 2.290167438318, avg_Acc: 64.380365030419
2017-12-28 10:58:22.828532: Iter: 74000 [6], loss: 0.020095, acc: 1.00%, avg_loss: 2.325256, avg_acc: 65.05%
2017-12-28 10:59:40.347627: Iter: 76000 [6], loss: 0.008328, acc: 1.00%, avg_loss: 2.256632, avg_acc: 65.85%
2017-12-28 11:00:57.792455: Iter: 78000 [6], loss: 0.000011, acc: 1.00%, avg_loss: 2.271614, avg_acc: 65.25%
2017-12-28 11:02:15.006072: Iter: 80000 [6], loss: 0.000067, acc: 1.00%, avg_loss: 2.270653, avg_acc: 64.99%
2017-12-28 11:03:32.103248: Iter: 82000 [6], loss: 4.954150, acc: 0.00%, avg_loss: 2.262902, avg_acc: 65.06%
2017-12-28 11:04:50.310398: Iter: 84000 [6], loss: 7.013886, acc: 0.00%, avg_loss: 2.264772, avg_acc: 64.96%
2017-12-28 11:04:50.310588: 
Epoch 6: avg_Loss: 2.264960722660, avg_Acc: 64.963746978915
2017-12-28 11:06:09.101110: Iter: 86000 [7], loss: 7.524930, acc: 0.00%, avg_loss: 2.328435, avg_acc: 64.80%
2017-12-28 11:07:27.701543: Iter: 88000 [7], loss: 0.018782, acc: 1.00%, avg_loss: 2.248144, avg_acc: 64.53%
2017-12-28 11:08:45.994699: Iter: 90000 [7], loss: 0.000001, acc: 1.00%, avg_loss: 2.258831, avg_acc: 64.77%
2017-12-28 11:10:03.654483: Iter: 92000 [7], loss: 0.000179, acc: 1.00%, avg_loss: 2.227383, avg_acc: 65.44%
2017-12-28 11:11:21.441435: Iter: 94000 [7], loss: 0.000931, acc: 1.00%, avg_loss: 2.212373, avg_acc: 65.38%
2017-12-28 11:12:39.284615: Iter: 96000 [7], loss: 8.382571, acc: 0.00%, avg_loss: 2.201448, avg_acc: 65.50%
2017-12-28 11:12:39.284811: 
Epoch 7: avg_Loss: 2.201631641028, avg_Acc: 65.505458788232
2017-12-28 11:12:40.039693: 
Testing at last epoch...
2017-12-28 11:19:05.878505: epoch: 8 Accuracy: 62.1500%, loss: 2.599416380949, i: 10000
2017-12-28 11:19:05.878565: Exiting train...
