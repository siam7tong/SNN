2017-12-28 15:06:16.752228: softmax classifer
2017-12-28 15:06:16.752334: Learning rates LR: 0.001000 
2017-12-28 15:07:40.539531: Iter: 2000 [0], loss: 0.029780, acc: 1.00%, avg_loss: 1.014265, avg_acc: 79.55%
2017-12-28 15:09:00.377737: Iter: 4000 [0], loss: 0.000005, acc: 1.00%, avg_loss: 1.011060, avg_acc: 79.07%
2017-12-28 15:10:19.816458: Iter: 6000 [0], loss: 0.005949, acc: 1.00%, avg_loss: 1.023445, avg_acc: 78.95%
2017-12-28 15:11:39.264676: Iter: 8000 [0], loss: 0.000008, acc: 1.00%, avg_loss: 1.033722, avg_acc: 79.03%
2017-12-28 15:12:59.120708: Iter: 10000 [0], loss: 4.678262, acc: 0.00%, avg_loss: 1.036282, avg_acc: 78.91%
2017-12-28 15:14:19.593398: Iter: 12000 [0], loss: 2.549423, acc: 0.00%, avg_loss: 1.022918, avg_acc: 79.16%
2017-12-28 15:14:19.593736: 
Epoch 0: avg_Loss: 1.023003053847, avg_Acc: 79.164930410868
2017-12-28 15:14:19.593772: Learning rates changed LR: 0.001000 
2017-12-28 15:15:46.843216: Iter: 14000 [1], loss: 2.252913, acc: 0.00%, avg_loss: 0.954719, avg_acc: 79.45%
2017-12-28 15:17:08.925907: Iter: 16000 [1], loss: 2.191958, acc: 0.00%, avg_loss: 0.991896, avg_acc: 79.55%
2017-12-28 15:18:33.251416: Iter: 18000 [1], loss: 0.382727, acc: 1.00%, avg_loss: 0.976380, avg_acc: 79.63%
2017-12-28 15:20:01.365534: Iter: 20000 [1], loss: 12.815363, acc: 0.00%, avg_loss: 1.002971, avg_acc: 79.64%
2017-12-28 15:21:26.465290: Iter: 22000 [1], loss: 0.000001, acc: 1.00%, avg_loss: 0.991330, avg_acc: 79.69%
2017-12-28 15:22:51.837669: Iter: 24000 [1], loss: 0.000405, acc: 1.00%, avg_loss: 1.022945, avg_acc: 79.12%
2017-12-28 15:22:51.837960: 
Epoch 1: avg_Loss: 1.023030595259, avg_Acc: 79.131594299525
2017-12-28 15:22:51.838003: Learning rates changed LR: 0.000100 
2017-12-28 15:24:19.703376: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1.031788, avg_acc: 79.70%
2017-12-28 15:25:41.687245: Iter: 28000 [2], loss: 0.000043, acc: 1.00%, avg_loss: 1.024163, avg_acc: 79.50%
2017-12-28 15:27:02.702814: Iter: 30000 [2], loss: 1.877147, acc: 0.00%, avg_loss: 1.012464, avg_acc: 79.47%
2017-12-28 15:28:22.972474: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1.014419, avg_acc: 79.39%
2017-12-28 15:29:46.050982: Iter: 34000 [2], loss: 0.000397, acc: 1.00%, avg_loss: 1.018583, avg_acc: 79.38%
2017-12-28 15:31:05.785080: Iter: 36000 [2], loss: 0.000053, acc: 1.00%, avg_loss: 1.021763, avg_acc: 79.17%
2017-12-28 15:31:05.785294: 
Epoch 2: avg_Loss: 1.021848065975, avg_Acc: 79.181598466539
2017-12-28 15:31:05.785325: Learning rates changed LR: 0.000100 
2017-12-28 15:32:25.608832: Iter: 38000 [3], loss: 0.000046, acc: 1.00%, avg_loss: 0.960591, avg_acc: 79.95%
2017-12-28 15:33:46.359392: Iter: 40000 [3], loss: 0.000275, acc: 1.00%, avg_loss: 1.015666, avg_acc: 79.45%
2017-12-28 15:35:08.140291: Iter: 42000 [3], loss: 0.000858, acc: 1.00%, avg_loss: 1.009113, avg_acc: 79.38%
2017-12-28 15:36:29.465171: Iter: 44000 [3], loss: 0.000003, acc: 1.00%, avg_loss: 1.027421, avg_acc: 78.83%
2017-12-28 15:37:50.556140: Iter: 46000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1.013598, avg_acc: 79.17%
2017-12-28 15:39:15.897567: Iter: 48000 [3], loss: 0.000346, acc: 1.00%, avg_loss: 1.021725, avg_acc: 79.15%
2017-12-28 15:39:15.897804: 
Epoch 3: avg_Loss: 1.021810449933, avg_Acc: 79.156596383032
2017-12-28 15:39:15.897834: Learning rates changed LR: 0.000010 
2017-12-28 15:40:37.032813: Iter: 50000 [4], loss: 0.002199, acc: 1.00%, avg_loss: 0.996019, avg_acc: 80.20%
2017-12-28 15:41:57.462590: Iter: 52000 [4], loss: 2.549711, acc: 0.00%, avg_loss: 1.036541, avg_acc: 79.15%
2017-12-28 15:43:21.065244: Iter: 54000 [4], loss: 0.005408, acc: 1.00%, avg_loss: 1.002310, avg_acc: 79.22%
2017-12-28 15:44:41.083895: Iter: 56000 [4], loss: 0.000007, acc: 1.00%, avg_loss: 1.017420, avg_acc: 78.92%
2017-12-28 15:46:01.195968: Iter: 58000 [4], loss: 4.081004, acc: 0.00%, avg_loss: 0.994704, avg_acc: 79.35%
2017-12-28 15:47:21.276725: Iter: 60000 [4], loss: 0.752993, acc: 0.00%, avg_loss: 1.021544, avg_acc: 79.15%
2017-12-28 15:47:21.276966: 
Epoch 4: avg_Loss: 1.021628674102, avg_Acc: 79.156596383032
2017-12-28 15:47:21.276998: Learning rates changed LR: 0.000010 
2017-12-28 15:48:47.527829: Iter: 62000 [5], loss: 3.477014, acc: 0.00%, avg_loss: 1.042363, avg_acc: 78.90%
2017-12-28 15:50:09.530071: Iter: 64000 [5], loss: 0.000408, acc: 1.00%, avg_loss: 1.043844, avg_acc: 79.25%
2017-12-28 15:51:32.721179: Iter: 66000 [5], loss: 0.000040, acc: 1.00%, avg_loss: 1.030039, avg_acc: 79.53%
2017-12-28 15:52:51.573091: Iter: 68000 [5], loss: 3.410969, acc: 0.00%, avg_loss: 1.045806, avg_acc: 79.05%
2017-12-28 15:54:10.247905: Iter: 70000 [5], loss: 0.000003, acc: 1.00%, avg_loss: 1.024928, avg_acc: 79.23%
2017-12-28 15:55:32.033016: Iter: 72000 [5], loss: 0.000027, acc: 1.00%, avg_loss: 1.021541, avg_acc: 79.15%
2017-12-28 15:55:32.033306: 
Epoch 5: avg_Loss: 1.021626523269, avg_Acc: 79.156596383032
2017-12-28 15:55:32.033341: Learning rates changed LR: 0.000001 
2017-12-28 15:56:54.894507: Iter: 74000 [6], loss: 0.000540, acc: 1.00%, avg_loss: 1.021506, avg_acc: 79.90%
2017-12-28 15:58:14.862431: Iter: 76000 [6], loss: 0.000003, acc: 1.00%, avg_loss: 1.025168, avg_acc: 79.07%
2017-12-28 15:59:35.341656: Iter: 78000 [6], loss: 0.001470, acc: 1.00%, avg_loss: 1.039075, avg_acc: 78.95%
2017-12-28 16:00:56.437332: Iter: 80000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1.041584, avg_acc: 78.84%
2017-12-28 16:02:15.957728: Iter: 82000 [6], loss: 0.000017, acc: 1.00%, avg_loss: 1.025311, avg_acc: 79.21%
2017-12-28 16:03:36.525114: Iter: 84000 [6], loss: 0.000478, acc: 1.00%, avg_loss: 1.021537, avg_acc: 79.15%
2017-12-28 16:03:36.525358: 
Epoch 6: avg_Loss: 1.021621892452, avg_Acc: 79.156596383032
2017-12-28 16:03:36.525388: Learning rates changed LR: 0.000001 
2017-12-28 16:05:04.683891: Iter: 86000 [7], loss: 0.000014, acc: 1.00%, avg_loss: 1.049163, avg_acc: 79.10%
2017-12-28 16:06:25.194251: Iter: 88000 [7], loss: 0.000016, acc: 1.00%, avg_loss: 1.041952, avg_acc: 79.15%
2017-12-28 16:07:50.442376: Iter: 90000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1.029492, avg_acc: 79.07%
2017-12-28 16:09:11.944950: Iter: 92000 [7], loss: 0.008122, acc: 1.00%, avg_loss: 1.038011, avg_acc: 78.86%
2017-12-28 16:10:34.222183: Iter: 94000 [7], loss: 0.000016, acc: 1.00%, avg_loss: 1.017243, avg_acc: 79.16%
2017-12-28 16:11:54.374191: Iter: 96000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1.021537, avg_acc: 79.15%
2017-12-28 16:11:54.374379: 
Epoch 7: avg_Loss: 1.021621892452, avg_Acc: 79.156596383032
2017-12-28 16:11:55.215822: 
Testing at last epoch...
2017-12-28 16:18:29.934725: epoch: 8 Accuracy: 79.6100%, loss: 0.988599250623, i: 10000
2017-12-28 16:18:29.934798: Exiting train...
