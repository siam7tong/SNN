2018-01-02 17:03:40.249875: softmax classifer
2018-01-02 17:03:40.249966: Learning rates LR: 0.000100 
2018-01-02 17:05:38.698649: Iter: 2000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 15154.845953, avg_acc: 76.80%
2018-01-02 17:07:36.691921: Iter: 4000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 15127.339248, avg_acc: 76.35%
2018-01-02 17:09:34.835659: Iter: 6000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 15244.007760, avg_acc: 76.13%
2018-01-02 17:11:32.665806: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 15318.597260, avg_acc: 76.21%
2018-01-02 17:13:30.679282: Iter: 10000 [0], loss: 84820.250000, acc: 0.00%, avg_loss: 15350.261981, avg_acc: 76.15%
2018-01-02 17:15:28.482569: Iter: 12000 [0], loss: 44377.246094, acc: 0.00%, avg_loss: 15169.003316, avg_acc: 76.33%
2018-01-02 17:15:28.483221: 
Epoch 0: avg_Loss: 15170.267504713935, avg_Acc: 76.331360946746
2018-01-02 17:15:28.483354: Learning rates changed LR: 0.000100 
2018-01-02 17:17:27.166536: Iter: 14000 [1], loss: 2355.500000, acc: 0.00%, avg_loss: 13966.487011, avg_acc: 76.85%
2018-01-02 17:19:24.858577: Iter: 16000 [1], loss: 22584.625000, acc: 0.00%, avg_loss: 14682.264646, avg_acc: 76.78%
2018-01-02 17:21:22.501935: Iter: 18000 [1], loss: 13275.234375, acc: 0.00%, avg_loss: 14403.475662, avg_acc: 76.93%
2018-01-02 17:23:20.062242: Iter: 20000 [1], loss: 109397.820312, acc: 0.00%, avg_loss: 14785.232690, avg_acc: 76.81%
2018-01-02 17:25:17.762008: Iter: 22000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 14668.086182, avg_acc: 76.84%
2018-01-02 17:27:15.706107: Iter: 24000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 15166.703934, avg_acc: 76.32%
2018-01-02 17:27:15.706464: 
Epoch 1: avg_Loss: 15167.967931246874, avg_Acc: 76.323026918910
2018-01-02 17:27:15.706598: Learning rates changed LR: 0.000100 
2018-01-02 17:29:14.587697: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15756.113624, avg_acc: 77.05%
2018-01-02 17:31:12.366120: Iter: 28000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15215.798648, avg_acc: 77.00%
2018-01-02 17:33:10.322618: Iter: 30000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15080.573517, avg_acc: 76.70%
2018-01-02 17:35:08.220075: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15039.811508, avg_acc: 76.54%
2018-01-02 17:37:06.158843: Iter: 34000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15113.563565, avg_acc: 76.53%
2018-01-02 17:39:04.225898: Iter: 36000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 15163.992838, avg_acc: 76.31%
2018-01-02 17:39:04.226254: 
Epoch 2: avg_Loss: 15165.256609437505, avg_Acc: 76.314692891074
2018-01-02 17:39:04.226379: Learning rates changed LR: 0.000100 
2018-01-02 17:41:03.058747: Iter: 38000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 14076.523296, avg_acc: 77.60%
2018-01-02 17:43:00.664472: Iter: 40000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 14906.577470, avg_acc: 76.80%
2018-01-02 17:44:58.152141: Iter: 42000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 14823.870813, avg_acc: 76.95%
2018-01-02 17:46:55.951398: Iter: 44000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 15145.480180, avg_acc: 76.42%
2018-01-02 17:48:53.535127: Iter: 46000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 14999.165513, avg_acc: 76.44%
2018-01-02 17:50:51.178792: Iter: 48000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 15161.700955, avg_acc: 76.32%
2018-01-02 17:50:51.179031: 
Epoch 3: avg_Loss: 15162.964535607782, avg_Acc: 76.323026918910
2018-01-02 17:50:51.179067: Learning rates changed LR: 0.000050 
2018-01-02 17:52:49.751438: Iter: 50000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 14644.309208, avg_acc: 77.00%
2018-01-02 17:54:47.727220: Iter: 52000 [4], loss: 13876.562500, acc: 0.00%, avg_loss: 15112.085935, avg_acc: 76.12%
2018-01-02 17:56:45.746988: Iter: 54000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 14901.628293, avg_acc: 76.43%
2018-01-02 17:58:43.894093: Iter: 56000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 15130.688296, avg_acc: 76.33%
2018-01-02 18:00:39.787936: Iter: 58000 [4], loss: 63084.742188, acc: 0.00%, avg_loss: 14793.873136, avg_acc: 76.58%
2018-01-02 18:01:57.839985: Iter: 60000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 15156.961062, avg_acc: 76.33%
2018-01-02 18:01:57.840238: 
Epoch 4: avg_Loss: 15158.224247589480, avg_Acc: 76.339694974581
2018-01-02 18:01:57.840276: Learning rates changed LR: 0.000050 
2018-01-02 18:03:16.769805: Iter: 62000 [5], loss: 52941.734375, acc: 0.00%, avg_loss: 15369.072194, avg_acc: 76.05%
2018-01-02 18:04:34.956753: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 15508.404938, avg_acc: 76.38%
2018-01-02 18:05:52.871110: Iter: 66000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 15203.077890, avg_acc: 76.52%
2018-01-02 18:07:10.803433: Iter: 68000 [5], loss: 37759.468750, acc: 0.00%, avg_loss: 15534.503614, avg_acc: 76.33%
2018-01-02 18:08:29.447623: Iter: 70000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 15259.377072, avg_acc: 76.39%
2018-01-02 18:09:47.247666: Iter: 72000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 15155.995691, avg_acc: 76.33%
2018-01-02 18:09:47.247889: 
Epoch 5: avg_Loss: 15157.258796346752, avg_Acc: 76.331360946746
2018-01-02 18:09:47.247927: Learning rates changed LR: 0.000050 
2018-01-02 18:09:47.997868: Exiting train...
