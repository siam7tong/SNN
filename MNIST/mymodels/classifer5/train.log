2017-12-20 17:09:37.617184: softmax classifer
2017-12-20 17:09:37.617266: Learning rates LR: 100.000000 
2017-12-20 17:10:55.283849: Iter: 2000 [0], loss: 27507632.000000, acc: 0.00%, avg_loss: 11960660.774221, avg_acc: 30.20%
2017-12-20 17:12:12.559405: Iter: 4000 [0], loss: 33463838.000000, acc: 0.00%, avg_loss: 11282453.052486, avg_acc: 34.08%
2017-12-20 17:13:31.010334: Iter: 6000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 11168517.811657, avg_acc: 36.38%
2017-12-20 17:14:49.406773: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 10684695.755055, avg_acc: 38.90%
2017-12-20 17:16:08.097394: Iter: 10000 [0], loss: 39370348.000000, acc: 0.00%, avg_loss: 10507300.724894, avg_acc: 40.76%
2017-12-20 17:17:26.688455: Iter: 12000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 10238929.140370, avg_acc: 42.58%
2017-12-20 17:17:26.688873: 
Epoch 0: avg_Loss: 10239782.455574829131, avg_Acc: 42.586882240187
2017-12-20 17:17:26.688903: Learning rates changed LR: 10.000000 
2017-12-20 17:18:44.128060: Iter: 14000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2747739.283500, avg_acc: 69.15%
2017-12-20 17:20:01.849405: Iter: 16000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2378788.526187, avg_acc: 71.23%
2017-12-20 17:21:19.317059: Iter: 18000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2322196.041250, avg_acc: 71.50%
2017-12-20 17:22:37.070116: Iter: 20000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2255415.084375, avg_acc: 71.84%
2017-12-20 17:23:55.429063: Iter: 22000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2238992.849275, avg_acc: 71.86%
2017-12-20 17:25:14.307928: Iter: 24000 [1], loss: 13404071.000000, acc: 0.00%, avg_loss: 2205700.409146, avg_acc: 71.90%
2017-12-20 17:25:14.308194: 
Epoch 1: avg_Loss: 2205884.232831902802, avg_Acc: 71.905992166014
2017-12-20 17:25:14.308228: Learning rates changed LR: 2.000000 
2017-12-20 17:26:32.135046: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1709790.691875, avg_acc: 75.50%
2017-12-20 17:27:48.936393: Iter: 28000 [2], loss: 6355737.000000, acc: 0.00%, avg_loss: 1695715.464875, avg_acc: 75.72%
2017-12-20 17:29:05.942560: Iter: 30000 [2], loss: 1755091.000000, acc: 0.00%, avg_loss: 1698102.361500, avg_acc: 75.45%
2017-12-20 17:30:22.890975: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1682473.191188, avg_acc: 75.20%
2017-12-20 17:31:39.824780: Iter: 34000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1729676.658625, avg_acc: 74.98%
2017-12-20 17:32:56.694179: Iter: 36000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1723691.023979, avg_acc: 74.96%
2017-12-20 17:32:56.694418: 
Epoch 2: avg_Loss: 1723834.676868905779, avg_Acc: 74.964580381698
2017-12-20 17:32:56.694448: Learning rates changed LR: 1.000000 
2017-12-20 17:34:14.505135: Iter: 38000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1533499.172750, avg_acc: 75.30%
2017-12-20 17:35:31.795551: Iter: 40000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1551938.017938, avg_acc: 75.48%
2017-12-20 17:36:48.556016: Iter: 42000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1665472.497458, avg_acc: 75.57%
2017-12-20 17:38:05.728462: Iter: 44000 [3], loss: 5026847.000000, acc: 0.00%, avg_loss: 1675149.239344, avg_acc: 75.64%
2017-12-20 17:39:23.167832: Iter: 46000 [3], loss: 12147720.000000, acc: 0.00%, avg_loss: 1684538.970525, avg_acc: 75.45%
2017-12-20 17:40:39.879955: Iter: 48000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1654910.270062, avg_acc: 75.57%
2017-12-20 17:40:39.880332: 
Epoch 3: avg_Loss: 1655048.190745062195, avg_Acc: 75.572964413701
2017-12-20 17:40:39.880363: Learning rates changed LR: 0.100000 
2017-12-20 17:41:58.727844: Iter: 50000 [4], loss: 2637022.000000, acc: 0.00%, avg_loss: 1784219.632875, avg_acc: 75.25%
2017-12-20 17:43:16.454592: Iter: 52000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1701904.860438, avg_acc: 75.50%
2017-12-20 17:44:34.165186: Iter: 54000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1656741.598333, avg_acc: 75.47%
2017-12-20 17:45:51.871730: Iter: 56000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1649916.996531, avg_acc: 75.54%
2017-12-20 17:47:09.311955: Iter: 58000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1658700.678725, avg_acc: 75.44%
2017-12-20 17:48:25.580412: Iter: 60000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1626257.201313, avg_acc: 75.68%
2017-12-20 17:48:25.580593: 
Epoch 4: avg_Loss: 1626392.734040336683, avg_Acc: 75.689640803400
2017-12-20 17:48:25.580621: Learning rates changed LR: 0.010000 
2017-12-20 17:49:44.009221: Iter: 62000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1530168.710750, avg_acc: 77.40%
2017-12-20 17:51:01.347548: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1540667.708250, avg_acc: 77.10%
2017-12-20 17:52:18.972311: Iter: 66000 [5], loss: 7943910.000000, acc: 0.00%, avg_loss: 1604228.183167, avg_acc: 76.18%
2017-12-20 17:53:35.403232: Iter: 68000 [5], loss: 2998290.000000, acc: 0.00%, avg_loss: 1621814.657609, avg_acc: 75.86%
2017-12-20 17:54:51.834808: Iter: 70000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1617576.095862, avg_acc: 75.95%
2017-12-20 17:56:08.274686: Iter: 72000 [5], loss: 5042578.000000, acc: 0.00%, avg_loss: 1614816.947635, avg_acc: 75.83%
2017-12-20 17:56:08.274907: 
Epoch 5: avg_Loss: 1614951.526929327520, avg_Acc: 75.831319276606
2017-12-20 17:56:08.274992: Learning rates changed LR: 0.001000 
2017-12-20 17:57:25.597906: Iter: 74000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1678939.834250, avg_acc: 75.30%
2017-12-20 17:58:42.578113: Iter: 76000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1605036.820563, avg_acc: 75.60%
2017-12-20 17:59:59.844151: Iter: 78000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1637100.598375, avg_acc: 75.33%
2017-12-20 18:01:17.133081: Iter: 80000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1615452.315625, avg_acc: 75.51%
2017-12-20 18:02:34.232262: Iter: 82000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1612461.264450, avg_acc: 75.58%
2017-12-20 18:03:51.314682: Iter: 84000 [6], loss: 10399742.000000, acc: 0.00%, avg_loss: 1614045.312896, avg_acc: 75.80%
2017-12-20 18:03:51.314897: 
Epoch 6: avg_Loss: 1614179.827881490113, avg_Acc: 75.806317193099
2017-12-20 18:03:51.314936: Learning rates changed LR: 0.000100 
2017-12-20 18:05:09.322173: Iter: 86000 [7], loss: 7938340.000000, acc: 0.00%, avg_loss: 1688607.228125, avg_acc: 74.95%
2017-12-20 18:06:26.265549: Iter: 88000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1633434.984562, avg_acc: 75.30%
2017-12-20 18:07:43.188154: Iter: 90000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1652769.207542, avg_acc: 75.22%
2017-12-20 18:08:59.775289: Iter: 92000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1639293.430250, avg_acc: 75.65%
2017-12-20 18:10:16.442289: Iter: 94000 [7], loss: 0.000000, acc: 1.00%, avg_loss: 1608586.444575, avg_acc: 75.82%
2017-12-20 18:11:33.069665: Iter: 96000 [7], loss: 4878682.000000, acc: 0.00%, avg_loss: 1613969.889896, avg_acc: 75.81%
2017-12-20 18:11:33.069880: 
Epoch 7: avg_Loss: 1614104.398595716339, avg_Acc: 75.814651220935
2017-12-20 18:11:33.069906: Learning rates changed LR: 0.000000 
2017-12-20 18:11:33.817406: 
Testing at last epoch...
2017-12-20 18:17:57.485721: epoch: 8 Accuracy: 76.4300%, loss: 1511130.693200000096, i: 10000
2017-12-20 18:17:57.485782: Exiting train...
