2017-12-20 16:04:14.678486: softmax classifer
2017-12-20 16:04:14.678573: Learning rates LR: 100.000000 
2017-12-20 16:05:31.831826: Iter: 2000 [0], loss: 27507632.000000, acc: 0.00%, avg_loss: 11960660.774221, avg_acc: 30.20%
2017-12-20 16:06:50.089813: Iter: 4000 [0], loss: 33463838.000000, acc: 0.00%, avg_loss: 11282453.052486, avg_acc: 34.08%
2017-12-20 16:08:07.619949: Iter: 6000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 11168517.811657, avg_acc: 36.38%
2017-12-20 16:09:25.956999: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 10684695.755055, avg_acc: 38.90%
2017-12-20 16:10:43.162854: Iter: 10000 [0], loss: 39370348.000000, acc: 0.00%, avg_loss: 10507300.724894, avg_acc: 40.76%
2017-12-20 16:12:01.347040: Iter: 12000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 10238929.140370, avg_acc: 42.58%
2017-12-20 16:12:01.347298: 
Epoch 0: avg_Loss: 10239782.455574829131, avg_Acc: 42.586882240187
2017-12-20 16:12:01.347336: Learning rates changed LR: 2.000000 
2017-12-20 16:13:19.736072: Iter: 14000 [1], loss: 948450.000000, acc: 0.00%, avg_loss: 3337549.704000, avg_acc: 68.10%
2017-12-20 16:14:37.538930: Iter: 16000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2551737.925125, avg_acc: 71.67%
2017-12-20 16:15:56.524296: Iter: 18000 [1], loss: 739830.500000, acc: 0.00%, avg_loss: 2372867.869500, avg_acc: 72.13%
2017-12-20 16:17:15.626112: Iter: 20000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2252049.889406, avg_acc: 72.66%
2017-12-20 16:18:33.931872: Iter: 22000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 2205052.187350, avg_acc: 72.92%
2017-12-20 16:19:51.909298: Iter: 24000 [1], loss: 11501520.000000, acc: 0.00%, avg_loss: 2163384.905083, avg_acc: 73.07%
2017-12-20 16:19:51.909532: 
Epoch 1: avg_Loss: 2163565.202183515299, avg_Acc: 73.072756063005
2017-12-20 16:19:51.909566: Learning rates changed LR: 1.000000 
2017-12-20 16:21:10.650602: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1870511.161000, avg_acc: 75.35%
2017-12-20 16:22:28.519202: Iter: 28000 [2], loss: 8130683.000000, acc: 0.00%, avg_loss: 1862004.840250, avg_acc: 75.30%
2017-12-20 16:23:46.603689: Iter: 30000 [2], loss: 145341.000000, acc: 0.00%, avg_loss: 1869487.322583, avg_acc: 74.95%
2017-12-20 16:25:04.583311: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1855318.208438, avg_acc: 74.65%
2017-12-20 16:26:22.809022: Iter: 34000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1907346.128750, avg_acc: 74.36%
2017-12-20 16:27:40.708484: Iter: 36000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 1900404.370583, avg_acc: 74.27%
2017-12-20 16:27:40.708745: 
Epoch 2: avg_Loss: 1900562.750812567770, avg_Acc: 74.272856071339
2017-12-20 16:27:40.708778: Learning rates changed LR: 0.100000 
2017-12-20 16:28:58.544385: Iter: 38000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1733495.275250, avg_acc: 74.45%
2017-12-20 16:30:16.612754: Iter: 40000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1736583.602375, avg_acc: 74.67%
2017-12-20 16:31:34.664284: Iter: 42000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1856163.007417, avg_acc: 74.57%
2017-12-20 16:32:52.683170: Iter: 44000 [3], loss: 6678875.000000, acc: 0.00%, avg_loss: 1867679.904438, avg_acc: 74.74%
2017-12-20 16:34:10.705812: Iter: 46000 [3], loss: 15967583.000000, acc: 0.00%, avg_loss: 1879485.328650, avg_acc: 74.62%
2017-12-20 16:35:28.196824: Iter: 48000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 1853823.360875, avg_acc: 74.68%
2017-12-20 16:35:28.197225: 
Epoch 3: avg_Loss: 1853977.859029919142, avg_Acc: 74.689557463122
2017-12-20 16:35:28.197255: Learning rates changed LR: 0.010000 
2017-12-20 16:36:46.361348: Iter: 50000 [4], loss: 2109048.000000, acc: 0.00%, avg_loss: 2006099.663500, avg_acc: 74.20%
2017-12-20 16:38:03.471829: Iter: 52000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1928250.690375, avg_acc: 74.38%
2017-12-20 16:39:21.438875: Iter: 54000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1875282.688917, avg_acc: 74.50%
2017-12-20 16:40:40.350834: Iter: 56000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1871976.913437, avg_acc: 74.55%
2017-12-20 16:42:01.929499: Iter: 58000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1882880.366800, avg_acc: 74.46%
2017-12-20 16:43:19.647227: Iter: 60000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 1846640.902792, avg_acc: 74.72%
2017-12-20 16:43:19.647432: 
Epoch 4: avg_Loss: 1846794.802358529763, avg_Acc: 74.731227602300
2017-12-20 16:43:19.647473: Learning rates changed LR: 0.001000 
2017-12-20 16:44:38.530588: Iter: 62000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1747475.244250, avg_acc: 76.05%
2017-12-20 16:45:56.469466: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1755724.027000, avg_acc: 75.92%
2017-12-20 16:47:14.417235: Iter: 66000 [5], loss: 7686224.000000, acc: 0.00%, avg_loss: 1828116.917583, avg_acc: 75.12%
2017-12-20 16:48:32.208656: Iter: 68000 [5], loss: 3568036.000000, acc: 0.00%, avg_loss: 1851216.360969, avg_acc: 74.67%
2017-12-20 16:49:53.295715: Iter: 70000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 1845548.989275, avg_acc: 74.90%
2017-12-20 16:51:12.164082: Iter: 72000 [5], loss: 7107129.000000, acc: 0.00%, avg_loss: 1845552.133042, avg_acc: 74.73%
2017-12-20 16:51:12.164332: 
Epoch 5: avg_Loss: 1845705.941870155744, avg_Acc: 74.739561630136
2017-12-20 16:51:12.164376: Learning rates changed LR: 0.000100 
2017-12-20 16:52:30.113792: Iter: 74000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1907439.368250, avg_acc: 74.60%
2017-12-20 16:53:47.438608: Iter: 76000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1827957.874500, avg_acc: 74.75%
2017-12-20 16:55:04.592201: Iter: 78000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1868279.209875, avg_acc: 74.37%
2017-12-20 16:56:21.680298: Iter: 80000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1848234.878031, avg_acc: 74.49%
2017-12-20 16:57:38.907224: Iter: 82000 [6], loss: 0.000000, acc: 1.00%, avg_loss: 1845320.365175, avg_acc: 74.46%
2017-12-20 16:58:55.935492: Iter: 84000 [6], loss: 12385145.000000, acc: 0.00%, avg_loss: 1845459.065146, avg_acc: 74.74%
2017-12-20 16:58:55.935721: 
Epoch 6: avg_Loss: 1845612.866218018113, avg_Acc: 74.747895657971
2017-12-20 16:58:55.935749: Learning rates changed LR: 0.000000 
2017-12-20 16:58:56.707469: 
Testing at last epoch...
2017-12-20 17:05:22.704787: epoch: 7 Accuracy: 75.4900%, loss: 1708312.556349999970, i: 10000
2017-12-20 17:05:22.704859: Exiting train...
