2018-01-02 16:48:04.991399: softmax classifer
2018-01-02 16:48:04.991470: Learning rates LR: 10.000000 
2018-01-02 16:49:27.495054: Iter: 2000 [0], loss: 991344.625000, acc: 0.00%, avg_loss: 859367.573620, avg_acc: 28.20%
2018-01-02 16:50:46.169581: Iter: 4000 [0], loss: 205199.500000, acc: 0.00%, avg_loss: 818640.623607, avg_acc: 34.17%
2018-01-02 16:52:04.822948: Iter: 6000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 817932.024201, avg_acc: 37.23%
2018-01-02 16:53:22.987160: Iter: 8000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 791215.416159, avg_acc: 40.06%
2018-01-02 16:54:41.364388: Iter: 10000 [0], loss: 429679.250000, acc: 0.00%, avg_loss: 785346.775721, avg_acc: 41.80%
2018-01-02 16:55:59.726826: Iter: 12000 [0], loss: 0.000000, acc: 1.00%, avg_loss: 778969.944478, avg_acc: 43.37%
2018-01-02 16:55:59.727157: 
Epoch 0: avg_Loss: 779034.864050275646, avg_Acc: 43.370280856738
2018-01-02 16:55:59.727196: Learning rates changed LR: 1.000000 
2018-01-02 16:57:18.610164: Iter: 14000 [1], loss: 185583.250000, acc: 0.00%, avg_loss: 263274.378688, avg_acc: 69.40%
2018-01-02 16:58:36.348044: Iter: 16000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 222167.487160, avg_acc: 71.53%
2018-01-02 16:59:53.619107: Iter: 18000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 212666.980565, avg_acc: 72.13%
2018-01-02 17:01:11.233564: Iter: 20000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 206225.021818, avg_acc: 72.17%
2018-01-02 17:02:31.646874: Iter: 22000 [1], loss: 0.000000, acc: 1.00%, avg_loss: 204666.422702, avg_acc: 72.34%
2018-01-02 17:03:56.120478: Iter: 24000 [1], loss: 959110.625000, acc: 0.00%, avg_loss: 201233.899277, avg_acc: 72.38%
2018-01-02 17:03:56.120791: 
Epoch 1: avg_Loss: 201250.670166524302, avg_Acc: 72.389365780482
2018-01-02 17:03:56.120829: Learning rates changed LR: 0.100000 
2018-01-02 17:05:55.258066: Iter: 26000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 153044.378484, avg_acc: 76.05%
2018-01-02 17:07:52.504227: Iter: 28000 [2], loss: 503639.812500, acc: 0.00%, avg_loss: 150206.224352, avg_acc: 76.42%
2018-01-02 17:09:49.457164: Iter: 30000 [2], loss: 101712.250000, acc: 0.00%, avg_loss: 149828.365057, avg_acc: 76.18%
2018-01-02 17:11:45.960336: Iter: 32000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 149297.862367, avg_acc: 75.91%
2018-01-02 17:13:42.585376: Iter: 34000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 153372.403316, avg_acc: 75.63%
2018-01-02 17:15:40.684563: Iter: 36000 [2], loss: 0.000000, acc: 1.00%, avg_loss: 152742.363594, avg_acc: 75.66%
2018-01-02 17:15:40.684817: 
Epoch 2: avg_Loss: 152755.093184848723, avg_Acc: 75.664638719893
2018-01-02 17:15:40.684855: Learning rates changed LR: 0.100000 
2018-01-02 17:17:37.981296: Iter: 38000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 139026.551172, avg_acc: 75.80%
2018-01-02 17:19:34.581844: Iter: 40000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 140326.017844, avg_acc: 76.17%
2018-01-02 17:21:30.873909: Iter: 42000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 150340.072234, avg_acc: 76.03%
2018-01-02 17:23:27.216521: Iter: 44000 [3], loss: 277026.375000, acc: 0.00%, avg_loss: 151260.033594, avg_acc: 76.09%
2018-01-02 17:25:23.510346: Iter: 46000 [3], loss: 1244807.125000, acc: 0.00%, avg_loss: 152029.580406, avg_acc: 75.94%
2018-01-02 17:27:19.935196: Iter: 48000 [3], loss: 0.000000, acc: 1.00%, avg_loss: 149427.550828, avg_acc: 76.07%
2018-01-02 17:27:19.935439: 
Epoch 3: avg_Loss: 149440.004161805147, avg_Acc: 76.073006083840
2018-01-02 17:27:19.935478: Learning rates changed LR: 0.010000 
2018-01-02 17:29:17.908843: Iter: 50000 [4], loss: 248254.750000, acc: 0.00%, avg_loss: 161101.892828, avg_acc: 75.70%
2018-01-02 17:31:15.075278: Iter: 52000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 153249.674453, avg_acc: 75.98%
2018-01-02 17:33:11.613759: Iter: 54000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 149170.658729, avg_acc: 76.03%
2018-01-02 17:35:08.376004: Iter: 56000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 147854.385660, avg_acc: 76.02%
2018-01-02 17:37:05.060465: Iter: 58000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 149070.441878, avg_acc: 76.02%
2018-01-02 17:39:01.968256: Iter: 60000 [4], loss: 0.000000, acc: 1.00%, avg_loss: 146523.630008, avg_acc: 76.24%
2018-01-02 17:39:01.968613: 
Epoch 4: avg_Loss: 146535.841327923146, avg_Acc: 76.248020668389
2018-01-02 17:39:01.968662: Learning rates changed LR: 0.001000 
2018-01-02 17:40:59.844205: Iter: 62000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 137358.727062, avg_acc: 77.45%
2018-01-02 17:42:56.889609: Iter: 64000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 139195.122031, avg_acc: 77.12%
2018-01-02 17:44:53.940024: Iter: 66000 [5], loss: 841280.000000, acc: 0.00%, avg_loss: 145147.330536, avg_acc: 76.17%
2018-01-02 17:46:51.322954: Iter: 68000 [5], loss: 497218.125000, acc: 0.00%, avg_loss: 146084.211516, avg_acc: 75.96%
2018-01-02 17:48:48.408264: Iter: 70000 [5], loss: 0.000000, acc: 1.00%, avg_loss: 145790.723563, avg_acc: 76.19%
2018-01-02 17:50:46.099864: Iter: 72000 [5], loss: 409401.750000, acc: 0.00%, avg_loss: 145451.376266, avg_acc: 76.16%
2018-01-02 17:50:46.100145: 
Epoch 5: avg_Loss: 145463.498223810311, avg_Acc: 76.164680390033
2018-01-02 17:50:46.100185: Learning rates changed LR: 0.000100 
2018-01-02 17:50:47.059748: 
Testing at last epoch...
2018-01-02 18:00:30.399887: epoch: 6 Accuracy: 76.8900%, loss: 137372.593246875011, i: 10000
2018-01-02 18:00:30.400185: Exiting train...
