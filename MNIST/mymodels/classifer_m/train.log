2017-12-28 20:21:13.367613: softmax classifer
2017-12-28 20:21:13.367697: Learning rates LR: 0.001000 
2017-12-28 20:22:37.775410: Iter: 2000 [0], loss: 4.983847, acc: 0.00%, avg_loss: 3.794774, avg_acc: 24.05%
2017-12-28 20:23:56.658717: Iter: 4000 [0], loss: 0.077202, acc: 1.00%, avg_loss: 3.250551, avg_acc: 30.73%
2017-12-28 20:25:15.148105: Iter: 6000 [0], loss: 0.292260, acc: 1.00%, avg_loss: 3.030028, avg_acc: 34.33%
2017-12-28 20:26:33.632486: Iter: 8000 [0], loss: 0.031143, acc: 1.00%, avg_loss: 2.839762, avg_acc: 37.14%
2017-12-28 20:27:51.723812: Iter: 10000 [0], loss: 0.307163, acc: 1.00%, avg_loss: 2.714880, avg_acc: 39.79%
2017-12-28 20:29:10.027911: Iter: 12000 [0], loss: 0.001876, acc: 1.00%, avg_loss: 2.625405, avg_acc: 41.76%
2017-12-28 20:29:10.028140: 
Epoch 0: avg_Loss: 2.625623767768, avg_Acc: 41.761813484457
2017-12-28 20:29:10.028166: Learning rates changed LR: 0.001000 
2017-12-28 20:30:28.674118: Iter: 14000 [1], loss: 0.007122, acc: 1.00%, avg_loss: 2.142659, avg_acc: 52.05%
2017-12-28 20:31:47.293621: Iter: 16000 [1], loss: 0.404353, acc: 1.00%, avg_loss: 1.957928, avg_acc: 54.55%
2017-12-28 20:33:05.477246: Iter: 18000 [1], loss: 5.591262, acc: 0.00%, avg_loss: 1.920172, avg_acc: 54.90%
2017-12-28 20:34:23.452897: Iter: 20000 [1], loss: 0.046143, acc: 1.00%, avg_loss: 1.879075, avg_acc: 55.54%
2017-12-28 20:35:40.647668: Iter: 22000 [1], loss: 1.409345, acc: 0.00%, avg_loss: 1.899648, avg_acc: 55.67%
2017-12-28 20:36:58.415546: Iter: 24000 [1], loss: 4.488861, acc: 0.00%, avg_loss: 1.898695, avg_acc: 55.90%
2017-12-28 20:36:58.415727: 
Epoch 1: avg_Loss: 1.898853324139, avg_Acc: 55.904658721560
2017-12-28 20:36:58.415753: Learning rates changed LR: 0.000100 
2017-12-28 20:38:16.594025: Iter: 26000 [2], loss: 0.000113, acc: 1.00%, avg_loss: 0.925350, avg_acc: 73.50%
2017-12-28 20:39:34.761172: Iter: 28000 [2], loss: 3.479992, acc: 0.00%, avg_loss: 0.925650, avg_acc: 73.08%
2017-12-28 20:40:51.832787: Iter: 30000 [2], loss: 0.705683, acc: 1.00%, avg_loss: 0.924663, avg_acc: 72.78%
2017-12-28 20:42:09.698866: Iter: 32000 [2], loss: 0.020350, acc: 1.00%, avg_loss: 0.920696, avg_acc: 72.56%
2017-12-28 20:43:26.526857: Iter: 34000 [2], loss: 0.019101, acc: 1.00%, avg_loss: 0.937460, avg_acc: 72.25%
2017-12-28 20:44:43.280236: Iter: 36000 [2], loss: 0.060753, acc: 1.00%, avg_loss: 0.934363, avg_acc: 72.22%
2017-12-28 20:44:43.280421: 
Epoch 2: avg_Loss: 0.934441113264, avg_Acc: 72.231019251604
2017-12-28 20:44:43.280456: Learning rates changed LR: 0.000100 
2017-12-28 20:46:01.709848: Iter: 38000 [3], loss: 1.971100, acc: 0.00%, avg_loss: 0.882549, avg_acc: 72.40%
2017-12-28 20:47:18.711262: Iter: 40000 [3], loss: 0.003469, acc: 1.00%, avg_loss: 0.877814, avg_acc: 72.62%
2017-12-28 20:48:35.089824: Iter: 42000 [3], loss: 0.000252, acc: 1.00%, avg_loss: 0.913117, avg_acc: 72.18%
2017-12-28 20:49:51.474568: Iter: 44000 [3], loss: 1.616104, acc: 0.00%, avg_loss: 0.909943, avg_acc: 72.52%
2017-12-28 20:51:07.952492: Iter: 46000 [3], loss: 2.884952, acc: 0.00%, avg_loss: 0.913064, avg_acc: 72.57%
2017-12-28 20:52:24.508132: Iter: 48000 [3], loss: 0.658724, acc: 1.00%, avg_loss: 0.899711, avg_acc: 72.69%
2017-12-28 20:52:24.508343: 
Epoch 3: avg_Loss: 0.899786390696, avg_Acc: 72.697724810401
2017-12-28 20:52:24.508373: Learning rates changed LR: 0.000010 
2017-12-28 20:53:42.473573: Iter: 50000 [4], loss: 0.903448, acc: 0.00%, avg_loss: 0.866000, avg_acc: 74.25%
2017-12-28 20:55:00.091067: Iter: 52000 [4], loss: 0.001001, acc: 1.00%, avg_loss: 0.842433, avg_acc: 75.15%
2017-12-28 20:56:17.083494: Iter: 54000 [4], loss: 0.003623, acc: 1.00%, avg_loss: 0.830826, avg_acc: 74.98%
2017-12-28 20:57:34.536101: Iter: 56000 [4], loss: 0.025117, acc: 1.00%, avg_loss: 0.824379, avg_acc: 74.67%
2017-12-28 20:58:51.168969: Iter: 58000 [4], loss: 0.969247, acc: 0.00%, avg_loss: 0.827550, avg_acc: 74.51%
2017-12-28 21:00:07.582899: Iter: 60000 [4], loss: 0.051392, acc: 1.00%, avg_loss: 0.818854, avg_acc: 74.67%
2017-12-28 21:00:07.583093: 
Epoch 4: avg_Loss: 0.818922333071, avg_Acc: 74.681223435286
2017-12-28 21:00:07.583127: Learning rates changed LR: 0.000010 
2017-12-28 21:01:25.046501: Iter: 62000 [5], loss: 0.000018, acc: 1.00%, avg_loss: 0.773766, avg_acc: 75.70%
2017-12-28 21:02:41.738728: Iter: 64000 [5], loss: 0.039140, acc: 1.00%, avg_loss: 0.781973, avg_acc: 75.48%
2017-12-28 21:03:58.690578: Iter: 66000 [5], loss: 1.846233, acc: 0.00%, avg_loss: 0.810031, avg_acc: 75.10%
2017-12-28 21:05:16.027064: Iter: 68000 [5], loss: 1.336602, acc: 0.00%, avg_loss: 0.816066, avg_acc: 74.84%
2017-12-28 21:06:33.444914: Iter: 70000 [5], loss: 0.159471, acc: 1.00%, avg_loss: 0.814165, avg_acc: 74.86%
2017-12-28 21:07:51.312112: Iter: 72000 [5], loss: 0.849177, acc: 1.00%, avg_loss: 0.813971, avg_acc: 74.88%
2017-12-28 21:07:51.312318: 
Epoch 5: avg_Loss: 0.814038853479, avg_Acc: 74.889574131178
2017-12-28 21:07:51.312351: Learning rates changed LR: 0.000001 
2017-12-28 21:09:09.485114: Iter: 74000 [6], loss: 0.178841, acc: 1.00%, avg_loss: 0.819854, avg_acc: 73.90%
2017-12-28 21:10:27.768909: Iter: 76000 [6], loss: 0.002493, acc: 1.00%, avg_loss: 0.802838, avg_acc: 74.10%
2017-12-28 21:11:48.609053: Iter: 78000 [6], loss: 0.001995, acc: 1.00%, avg_loss: 0.809666, avg_acc: 74.17%
2017-12-28 21:13:06.826651: Iter: 80000 [6], loss: 0.007769, acc: 1.00%, avg_loss: 0.809071, avg_acc: 74.52%
2017-12-28 21:14:25.443627: Iter: 82000 [6], loss: 0.515152, acc: 1.00%, avg_loss: 0.806459, avg_acc: 74.74%
2017-12-28 21:15:43.075741: Iter: 84000 [6], loss: 1.644503, acc: 0.00%, avg_loss: 0.807396, avg_acc: 74.83%
2017-12-28 21:15:43.075970: 
Epoch 6: avg_Loss: 0.807462969225, avg_Acc: 74.831235936328
2017-12-28 21:15:43.076007: Learning rates changed LR: 0.000001 
2017-12-28 21:17:01.156291: Iter: 86000 [7], loss: 5.133445, acc: 0.00%, avg_loss: 0.829155, avg_acc: 74.60%
2017-12-28 21:18:18.373834: Iter: 88000 [7], loss: 0.005410, acc: 1.00%, avg_loss: 0.813120, avg_acc: 74.83%
2017-12-28 21:19:36.445081: Iter: 90000 [7], loss: 0.005122, acc: 1.00%, avg_loss: 0.821412, avg_acc: 74.70%
2017-12-28 21:20:54.127299: Iter: 92000 [7], loss: 0.290854, acc: 1.00%, avg_loss: 0.814112, avg_acc: 74.83%
2017-12-28 21:22:11.994655: Iter: 94000 [7], loss: 0.002691, acc: 1.00%, avg_loss: 0.806096, avg_acc: 74.87%
2017-12-28 21:23:28.719129: Iter: 96000 [7], loss: 1.592503, acc: 0.00%, avg_loss: 0.805808, avg_acc: 74.95%
2017-12-28 21:23:28.719332: 
Epoch 7: avg_Loss: 0.805874949205, avg_Acc: 74.956246353863
2017-12-28 21:23:29.482687: 
Testing at last epoch...
2017-12-28 21:29:51.854886: epoch: 8 Accuracy: 75.5500%, loss: 0.781840494581, i: 10000
2017-12-28 21:29:51.854949: Exiting train...
